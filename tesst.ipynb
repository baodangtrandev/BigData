{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2235372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6070a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL to Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f579bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.parquet('24_03.parquet')\n",
    "df2 = spark.read.parquet('24_04.parquet')\n",
    "\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf8e0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- jid: string (nullable = true)\n",
      " |-- usr: string (nullable = true)\n",
      " |-- jnam: string (nullable = true)\n",
      " |-- cnumr: long (nullable = true)\n",
      " |-- cnumat: long (nullable = true)\n",
      " |-- cnumut: long (nullable = true)\n",
      " |-- nnumr: long (nullable = true)\n",
      " |-- adt: string (nullable = true)\n",
      " |-- qdt: string (nullable = true)\n",
      " |-- schedsdt: string (nullable = true)\n",
      " |-- deldt: string (nullable = true)\n",
      " |-- ec: long (nullable = true)\n",
      " |-- elpl: double (nullable = true)\n",
      " |-- sdt: string (nullable = true)\n",
      " |-- edt: string (nullable = true)\n",
      " |-- nnuma: long (nullable = true)\n",
      " |-- idle_time_ave: double (nullable = true)\n",
      " |-- nnumu: long (nullable = true)\n",
      " |-- perf1: double (nullable = true)\n",
      " |-- perf2: double (nullable = true)\n",
      " |-- perf3: double (nullable = true)\n",
      " |-- perf4: double (nullable = true)\n",
      " |-- perf5: double (nullable = true)\n",
      " |-- perf6: double (nullable = true)\n",
      " |-- mszl: double (nullable = true)\n",
      " |-- pri: long (nullable = true)\n",
      " |-- econ: double (nullable = true)\n",
      " |-- avgpcon: double (nullable = true)\n",
      " |-- minpcon: double (nullable = true)\n",
      " |-- maxpcon: double (nullable = true)\n",
      " |-- msza: decimal(20,0) (nullable = true)\n",
      " |-- mmszu: double (nullable = true)\n",
      " |-- uctmut: double (nullable = true)\n",
      " |-- sctmut: double (nullable = true)\n",
      " |-- usctmut: double (nullable = true)\n",
      " |-- jobenv_req: string (nullable = true)\n",
      " |-- freq_req: long (nullable = true)\n",
      " |-- freq_alloc: long (nullable = true)\n",
      " |-- flops: double (nullable = true)\n",
      " |-- mbwidth: double (nullable = true)\n",
      " |-- opint: double (nullable = true)\n",
      " |-- pclass: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- exit state: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0880f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract anonymous integers\n",
    "def extract_anon_int(col_name):\n",
    "    return when(\n",
    "        col(col_name).isNotNull() & col(col_name).rlike(\"_\\\\d+$\"),\n",
    "        regexp_extract(col(col_name), \"_(\\\\d+)$\", 1).cast(\"int\")\n",
    "    ).otherwise(0)\n",
    "\n",
    "df = df.withColumn(\"jnam_anon\", extract_anon_int(\"jnam\")) \\\n",
    "       .withColumn(\"usr_anon\", extract_anon_int(\"usr\")) \\\n",
    "       .withColumn(\"jobenv_anon\", extract_anon_int(\"jobenv_req\"))\n",
    "\n",
    "# Create targets\n",
    "df = df.withColumn(\"target_ec\", when(col(\"exit state\") == \"completed\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"target_pclass\", when(col(\"pclass\") == \"compute-bound\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"target_avgpcon\", \n",
    "                   when((col(\"nnuma\") > 0) & (col(\"avgpcon\").isNotNull()), \n",
    "                        (col(\"avgpcon\") / col(\"nnuma\")).cast(\"double\")).otherwise(lit(None))) \\\n",
    "       .withColumn(\"target_duration\",\n",
    "                   when(col(\"duration\").isNotNull(), (col(\"duration\") / 60.0).cast(\"double\")).otherwise(lit(None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    \"cnumr\", \"cnumat\", \"cnumut\", \"nnumr\", \"elpl\", \"nnuma\",\n",
    "    \"idle_time_ave\", \"nnumu\", \"perf1\", \"perf2\", \"perf3\", \"perf4\",\n",
    "    \"perf5\", \"perf6\", \"mszl\", \"pri\", \"econ\", \"minpcon\",\n",
    "    \"maxpcon\", \"msza\", \"mmszu\", \"uctmut\", \"sctmut\", \"usctmut\",\n",
    "    \"freq_req\", \"freq_alloc\", \"flops\", \"mbwidth\", \"opint\",\n",
    "    \"jnam_anon\", \"usr_anon\", \"jobenv_anon\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fa3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Step 2: Defining tasks...\n",
      "   Tasks: 4\n",
      "   Features: 32\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. DEFINE TASKS\n",
    "# ============================================================================\n",
    "print(\"\\nStep 2: Defining tasks...\")\n",
    "\n",
    "tasks = {\n",
    "    \"ec\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"target\": \"target_ec\",\n",
    "        \"description\": \"Exit Code (completed vs not)\"\n",
    "    },\n",
    "    \"pclass\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"target\": \"target_pclass\",\n",
    "        \"description\": \"PClass (compute-bound vs others)\"\n",
    "    },\n",
    "    \"avgpcon\": {\n",
    "        \"type\": \"regression\",\n",
    "        \"target\": \"target_avgpcon\",\n",
    "        \"description\": \"AvgPCon per node\"\n",
    "    },\n",
    "    \"duration\": {\n",
    "        \"type\": \"regression\",\n",
    "        \"target\": \"target_duration\",\n",
    "        \"description\": \"Duration (minutes)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "feature_cols = numeric_cols  \n",
    "\n",
    "print(f\"Tasks: {len(tasks)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classification_models = {\n",
    "    \"Logistic Regression\": lambda: LogisticRegression(maxIter=100, regParam=0.01, elasticNetParam=0.5),\n",
    "    \"Random Forest\": lambda: RandomForestClassifier(numTrees=100, maxDepth=10, seed=42),\n",
    "    \"Gradient Boosting\": lambda: GBTClassifier(maxIter=100, maxDepth=5, seed=42)\n",
    "}\n",
    "\n",
    "regression_models = {\n",
    "    \"Linear Regression\": lambda: LinearRegression(maxIter=100, regParam=0.01, elasticNetParam=0.5),\n",
    "    \n",
    "    # Random Forest vá»›i config nháº¹ hÆ¡n\n",
    "    \"Random Forest\": lambda: RandomForestRegressor(\n",
    "        numTrees=30,           \n",
    "        maxDepth=6,            \n",
    "        maxBins=32,\n",
    "        minInstancesPerNode=20,\n",
    "        subsamplingRate=0.8,   \n",
    "        seed=42\n",
    "    ),\n",
    "    \n",
    "    \"GBT Regression\": lambda: GBTRegressor(\n",
    "        maxIter=50,    \n",
    "        maxDepth=4,    \n",
    "        stepSize=0.1,\n",
    "        seed=42\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_task(df, task_name, target_col, models_dict):\n",
    "    \n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"CLASSIFICATION: {task_name.upper()}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df_task = df.select(feature_cols + [target_col]) \\\n",
    "                .filter(col(target_col).isNotNull()) \\\n",
    "                .withColumnRenamed(target_col, \"label\")\n",
    "    \n",
    "    train_df, test_df = df_task.randomSplit([0.8, 0.2], seed=42)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    \n",
    "    print(f\"   Train: {train_df.count()} | Test: {test_df.count()}\")\n",
    "    \n",
    "    # Feature transformation\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "    scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "    \n",
    "    # Evaluators\n",
    "    eval_auc = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "    eval_acc = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    eval_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "    \n",
    "    # Train models\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for i, (model_name, model_fn) in enumerate(models_dict.items(), 1):\n",
    "        print(f\"\\n[{i}/{len(models_dict)}] Training {model_name}...\")\n",
    "        \n",
    "        model = model_fn()\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model])\n",
    "        trained_model = pipeline.fit(train_df)\n",
    "        predictions = trained_model.transform(test_df)\n",
    "        \n",
    "        auc = eval_auc.evaluate(predictions)\n",
    "        acc = eval_acc.evaluate(predictions)\n",
    "        f1 = eval_f1.evaluate(predictions)\n",
    "        \n",
    "        print(f\"AUC: {auc:.4f} | Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "        \n",
    "        results[model_name] = {\"AUC\": auc, \"Accuracy\": acc, \"F1\": f1}\n",
    "        trained_models[model_name] = trained_model\n",
    "    \n",
    "    # Cleanup\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n",
    "    \n",
    "    return results, trained_models\n",
    "\n",
    "\n",
    "def train_regression_task(df, task_name, target_col, models_dict):\n",
    "    \n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"REGRESSION: {task_name.upper()}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df_task = df.select(feature_cols + [target_col]) \\\n",
    "                .filter(col(target_col).isNotNull()) \\\n",
    "                .withColumnRenamed(target_col, \"label\")\n",
    "    \n",
    "    train_df, test_df = df_task.randomSplit([0.8, 0.2], seed=42)\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    \n",
    "    print(f\"   Train: {train_df.count()} | Test: {test_df.count()}\")\n",
    "    \n",
    "    # Feature transformation\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "    scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "    \n",
    "    # Evaluators\n",
    "    eval_rmse = RegressionEvaluator(metricName=\"rmse\")\n",
    "    eval_mae = RegressionEvaluator(metricName=\"mae\")\n",
    "    eval_r2 = RegressionEvaluator(metricName=\"r2\")\n",
    "    \n",
    "    # Train models\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for i, (model_name, model_fn) in enumerate(models_dict.items(), 1):\n",
    "        print(f\"\\n   [{i}/{len(models_dict)}] Training {model_name}...\")\n",
    "        \n",
    "        model = model_fn()\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model])\n",
    "        trained_model = pipeline.fit(train_df)\n",
    "        predictions = trained_model.transform(test_df)\n",
    "        \n",
    "        rmse = eval_rmse.evaluate(predictions)\n",
    "        mae = eval_mae.evaluate(predictions)\n",
    "        r2 = eval_r2.evaluate(predictions)\n",
    "        \n",
    "        print(f\"RMSE: {rmse:.4f} | MAE: {mae:.4f} | RÂ²: {r2:.4f}\")\n",
    "        \n",
    "        results[model_name] = {\"RMSE\": rmse, \"MAE\": mae, \"RÂ²\": r2}\n",
    "        trained_models[model_name] = trained_model\n",
    "    \n",
    "    # Cleanup\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n",
    "    \n",
    "    return results, trained_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import builtins  \n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Táº¡o thÆ° má»¥c results vÃ  checkpoints\n",
    "results_dir = \"D:/BigData/results\"\n",
    "checkpoints_dir = \"D:/BigData/checkpoints\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "training_times = {}\n",
    "\n",
    "# Training vá»›i timing\n",
    "for task_id, task_info in tasks.items():\n",
    "    task_type = task_info[\"type\"]\n",
    "    target_col = task_info[\"target\"]\n",
    "    description = task_info[\"description\"]\n",
    "    \n",
    "    print(f\"\\nStarting task: {task_id} ({description})\")\n",
    "    task_start_time = time.time()\n",
    "    \n",
    "    if task_type == \"classification\":\n",
    "        results, models = train_classification_task(\n",
    "            df, description, target_col, classification_models\n",
    "        )\n",
    "    else:\n",
    "        results, models = train_regression_task(\n",
    "            df, description, target_col, regression_models\n",
    "        )\n",
    "    \n",
    "    task_end_time = time.time()\n",
    "    task_duration = task_end_time - task_start_time\n",
    "    \n",
    "    all_results[task_id] = results\n",
    "    all_models[task_id] = models\n",
    "    training_times[task_id] = {\n",
    "        'duration_seconds': task_duration,\n",
    "        'duration_minutes': task_duration / 60,\n",
    "        'start_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(task_start_time)),\n",
    "        'end_time': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(task_end_time))\n",
    "    }\n",
    "    \n",
    "    print(f\"Task completed in {task_duration:.2f} seconds ({task_duration/60:.2f} minutes)\")\n",
    "    \n",
    "    # Save checkpoints for each task (PySpark models)\n",
    "    task_checkpoint_dir = os.path.join(checkpoints_dir, f\"{task_id}_{timestamp}\")\n",
    "    os.makedirs(task_checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    for model_name, model_pipeline in models.items():\n",
    "        if model_pipeline is not None:\n",
    "            try:\n",
    "                # Save PySpark model pipeline\n",
    "                model_path = os.path.join(task_checkpoint_dir, f\"{model_name.replace(' ', '_').lower()}\")\n",
    "                model_pipeline.write().overwrite().save(model_path)\n",
    "                print(f\"   ðŸ’¾ Saved PySpark model: {model_name} -> {model_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {model_name}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Checkpoints saved to: {task_checkpoint_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. EXPORT COMPREHENSIVE REPORTS\n",
    "# ============================================================================\n",
    "print(\"\\nExporting comprehensive results...\")\n",
    "\n",
    "# Export timing summary\n",
    "timing_file = f\"{results_dir}/training_times_{timestamp}.txt\"\n",
    "with open(timing_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"TRAINING TIME SUMMARY\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    total_time = builtins.sum(times['duration_seconds'] for times in training_times.values())\n",
    "    f.write(f\"Total Training Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\\n\\n\")\n",
    "    \n",
    "    for task_id, times in training_times.items():\n",
    "        task_info = tasks[task_id]\n",
    "        f.write(f\"{task_info['description']}:\\n\")\n",
    "        f.write(f\"  Start Time: {times['start_time']}\\n\")\n",
    "        f.write(f\"  End Time: {times['end_time']}\\n\")\n",
    "        f.write(f\"  Duration: {times['duration_seconds']:.2f} seconds ({times['duration_minutes']:.2f} minutes)\\n\\n\")\n",
    "\n",
    "print(f\"Timing report: {timing_file}\")\n",
    "\n",
    "# Export detailed reports for all tasks\n",
    "for task_id, task_info in tasks.items():\n",
    "    task_type = task_info[\"type\"]\n",
    "    target_col = task_info[\"target\"]\n",
    "    \n",
    "    # File path\n",
    "    report_file = f\"{results_dir}/{task_id}_detailed_report_{timestamp}.txt\"\n",
    "    \n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"DETAILED REPORT: {task_info['description']}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        # Training time info\n",
    "        times = training_times[task_id]\n",
    "        f.write(f\"Training Time: {times['duration_seconds']:.2f} seconds ({times['duration_minutes']:.2f} minutes)\\n\")\n",
    "        f.write(f\"Start: {times['start_time']} | End: {times['end_time']}\\n\\n\")\n",
    "        \n",
    "        # Model performance\n",
    "        results = all_results[task_id]\n",
    "        models = all_models[task_id]\n",
    "        \n",
    "        if task_type == \"classification\":\n",
    "            f.write(\"CLASSIFICATION PERFORMANCE\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(f\"{'Model':<20} {'AUC':<10} {'Accuracy':<12} {'F1':<10}\\n\")\n",
    "            f.write(\"-\" * 52 + \"\\n\")\n",
    "            \n",
    "            for model_name, metrics in results.items():\n",
    "                f.write(f\"{model_name:<20} {metrics['AUC']:<10.4f} {metrics['Accuracy']:<12.4f} {metrics['F1']:<10.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\\nDETAILED CLASSIFICATION REPORTS\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            \n",
    "            # Generate detailed classification reports for PySpark models\n",
    "            for model_name, model_pipeline in models.items():\n",
    "                if model_pipeline is None:\n",
    "                    f.write(f\"\\n{model_name}: TRAINING FAILED\\n\")\n",
    "                    continue\n",
    "                \n",
    "                f.write(f\"\\n{model_name}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                \n",
    "                try:\n",
    "                    # Recreate test data for this model\n",
    "                    df_task = df.select(feature_cols + [target_col]) \\\n",
    "                                .filter(col(target_col).isNotNull()) \\\n",
    "                                .withColumnRenamed(target_col, \"label\")\n",
    "                    \n",
    "                    train_df, test_df = df_task.randomSplit([0.8, 0.2], seed=42)\n",
    "                    test_df.cache()\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    predictions = model_pipeline.transform(test_df)\n",
    "                    pred_pandas = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "                    \n",
    "                    \n",
    "                    if len(pred_pandas) > 0:\n",
    "                        report = classification_report(\n",
    "                            pred_pandas[\"label\"], \n",
    "                            pred_pandas[\"prediction\"],\n",
    "                            target_names=['Class 0', 'Class 1'],\n",
    "                            zero_division=0\n",
    "                        )\n",
    "                        f.write(report)\n",
    "                        \n",
    "                        from sklearn.metrics import confusion_matrix\n",
    "                        cm = confusion_matrix(pred_pandas[\"label\"], pred_pandas[\"prediction\"])\n",
    "                        f.write(f\"\\nConfusion Matrix:\\n\")\n",
    "                        f.write(f\"True\\\\Pred    0    1\\n\")\n",
    "                        if cm.shape == (2, 2):\n",
    "                            f.write(f\"    0     {cm[0,0]:4d} {cm[0,1]:4d}\\n\")\n",
    "                            f.write(f\"    1     {cm[1,0]:4d} {cm[1,1]:4d}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"Confusion matrix shape: {cm.shape}\\n{cm}\\n\")\n",
    "                    else:\n",
    "                        f.write(\"No predictions available for detailed report\\n\")\n",
    "                    \n",
    "                    test_df.unpersist()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    f.write(f\"Error generating detailed classification report: {str(e)}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        else:  # Regression\n",
    "            f.write(\"REGRESSION PERFORMANCE\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(f\"{'Model':<20} {'RMSE':<12} {'MAE':<12} {'RÂ²':<10}\\n\")\n",
    "            f.write(\"-\" * 54 + \"\\n\")\n",
    "            \n",
    "            for model_name, metrics in results.items():\n",
    "                f.write(f\"{model_name:<20} {metrics['RMSE']:<12.4f} {metrics['MAE']:<12.4f} {metrics['RÂ²']:<10.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\\nDETAILED REGRESSION ANALYSIS\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            \n",
    "            # Generate detailed regression reports for PySpark models\n",
    "            for model_name, model_pipeline in models.items():\n",
    "                if model_pipeline is None:\n",
    "                    f.write(f\"\\n{model_name}: TRAINING FAILED\\n\")\n",
    "                    continue\n",
    "                \n",
    "                f.write(f\"\\n{model_name}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                \n",
    "                try:\n",
    "                    # Recreate test data for this model\n",
    "                    df_task = df.select(feature_cols + [target_col]) \\\n",
    "                                .filter(col(target_col).isNotNull()) \\\n",
    "                                .withColumnRenamed(target_col, \"label\")\n",
    "                    \n",
    "                    train_df, test_df = df_task.randomSplit([0.8, 0.2], seed=42)\n",
    "                    test_df.cache()\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    predictions = model_pipeline.transform(test_df)\n",
    "                    pred_pandas = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "                    \n",
    "                    if len(pred_pandas) > 0:\n",
    "                        y_test = pred_pandas[\"label\"].values\n",
    "                        y_pred = pred_pandas[\"prediction\"].values\n",
    "                        \n",
    "                        # Calculate detailed regression metrics\n",
    "                        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "                        import numpy as np\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                        mae = mean_absolute_error(y_test, y_pred)\n",
    "                        r2 = r2_score(y_test, y_pred)\n",
    "                        \n",
    "                        mask = y_test != 0\n",
    "                        if mask.any():\n",
    "                            mape = np.mean(np.abs((y_test[mask] - y_pred[mask]) / y_test[mask])) * 100\n",
    "                        else:\n",
    "                            mape = float('inf')\n",
    "                        \n",
    "                        f.write(f\"Root Mean Square Error (RMSE): {rmse:.4f}\\n\")\n",
    "                        f.write(f\"Mean Absolute Error (MAE): {mae:.4f}\\n\")\n",
    "                        f.write(f\"R-squared (RÂ²): {r2:.4f}\\n\")\n",
    "                        if mape != float('inf'):\n",
    "                            f.write(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\\n\")\n",
    "                        \n",
    "                        # Prediction statistics\n",
    "                        f.write(f\"\\nPrediction Statistics:\\n\")\n",
    "                        f.write(f\"  Actual - Min: {y_test.min():.4f}, Max: {y_test.max():.4f}, Mean: {y_test.mean():.4f}\\n\")\n",
    "                        f.write(f\"  Predicted - Min: {y_pred.min():.4f}, Max: {y_pred.max():.4f}, Mean: {y_pred.mean():.4f}\\n\")\n",
    "                        \n",
    "                        # Residual analysis\n",
    "                        residuals = y_test - y_pred\n",
    "                        f.write(f\"  Residuals - Min: {residuals.min():.4f}, Max: {residuals.max():.4f}, Mean: {residuals.mean():.4f}\\n\")\n",
    "                        f.write(f\"  Residuals Std: {residuals.std():.4f}\\n\")\n",
    "                        \n",
    "                        # Additional metrics\n",
    "                        f.write(f\"\\nAdditional Metrics:\\n\")\n",
    "                        f.write(f\"  Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.4f}\\n\")\n",
    "                        f.write(f\"  Explained Variance: {1 - np.var(residuals) / np.var(y_test):.4f}\\n\")\n",
    "                        \n",
    "                    else:\n",
    "                        f.write(\"No predictions available for detailed report\\n\")\n",
    "                    \n",
    "                    test_df.unpersist()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    f.write(f\"Error calculating regression metrics: {str(e)}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"{task_id}: {report_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. EXPORT SUMMARY AND BEST MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to find best models safely\n",
    "def find_best_classification_model(results):\n",
    "    \"\"\"Find best classification model by AUC\"\"\"\n",
    "    if not results:\n",
    "        return None\n",
    "    best_auc = -1\n",
    "    best_model = None\n",
    "    for model_name, metrics in results.items():\n",
    "        if metrics['AUC'] > best_auc:\n",
    "            best_auc = metrics['AUC']\n",
    "            best_model = (model_name, metrics)\n",
    "    return best_model\n",
    "\n",
    "def find_best_regression_model(results):\n",
    "    \"\"\"Find best regression model by lowest RMSE\"\"\"\n",
    "    if not results:\n",
    "        return None\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    for model_name, metrics in results.items():\n",
    "        if metrics['RMSE'] < best_rmse:\n",
    "            best_rmse = metrics['RMSE']\n",
    "            best_model = (model_name, metrics)\n",
    "    return best_model\n",
    "\n",
    "# Export overall summary\n",
    "summary_file = f\"{results_dir}/overall_summary_{timestamp}.txt\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"OVERALL TRAINING SUMMARY\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    # Use builtin sum explicitly\n",
    "    total_time_seconds = builtins.sum(times['duration_seconds'] for times in training_times.values())\n",
    "    \n",
    "    f.write(f\"Training Session: {timestamp}\\n\")\n",
    "    f.write(f\"Total Training Time: {total_time_seconds:.2f} seconds ({total_time_seconds/60:.2f} minutes)\\n\")\n",
    "    f.write(f\"Number of Tasks: {builtins.len(tasks)}\\n\")\n",
    "    f.write(f\"Results Directory: {results_dir}\\n\")\n",
    "    f.write(f\"Checkpoints Directory: {checkpoints_dir}\\n\\n\")\n",
    "    \n",
    "    f.write(\"BEST PERFORMING MODELS\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    for task_id, task_info in tasks.items():\n",
    "        results = all_results[task_id]\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        f.write(f\"\\n{task_info['description']}:\\n\")\n",
    "        \n",
    "        if task_info[\"type\"] == \"classification\":\n",
    "            # Find best model by AUC using helper function\n",
    "            try:\n",
    "                best_model = find_best_classification_model(results)\n",
    "                if best_model:\n",
    "                    model_name, metrics = best_model\n",
    "                    f.write(f\"Best Model: {model_name}\\n\")\n",
    "                    f.write(f\"AUC: {metrics['AUC']:.4f} | Accuracy: {metrics['Accuracy']:.4f} | F1: {metrics['F1']:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"No valid models found\\n\")\n",
    "            except Exception as e:\n",
    "                f.write(f\"Error finding best model: {str(e)}\\n\")\n",
    "        else:\n",
    "            # Find best model by lowest RMSE using helper function\n",
    "            try:\n",
    "                best_model = find_best_regression_model(results)\n",
    "                if best_model:\n",
    "                    model_name, metrics = best_model\n",
    "                    f.write(f\"Best Model: {model_name}\\n\")\n",
    "                    f.write(f\"RMSE: {metrics['RMSE']:.4f} | MAE: {metrics['MAE']:.4f} | RÂ²: {metrics['RÂ²']:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"No valid models found\\n\")\n",
    "            except Exception as e:\n",
    "                f.write(f\"Error finding best model: {str(e)}\\n\")\n",
    "        \n",
    "        f.write(f\"Training Time: {training_times[task_id]['duration_minutes']:.2f} minutes\\n\")\n",
    "\n",
    "print(f\"Overall summary: {summary_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. SAVE TRAINING METADATA\n",
    "# ============================================================================\n",
    "\n",
    "# Save metadata as pickle for easy loading (exclude PySpark objects)\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'training_times': training_times,\n",
    "    'all_results': all_results,\n",
    "    'tasks': tasks,\n",
    "    'feature_cols': feature_cols,\n",
    "    'total_records': df.count(),\n",
    "    'results_dir': results_dir,\n",
    "    'checkpoints_dir': checkpoints_dir\n",
    "}\n",
    "\n",
    "metadata_file = f\"{results_dir}/training_metadata_{timestamp}.pkl\"\n",
    "try:\n",
    "    with open(metadata_file, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"Metadata saved: {metadata_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving metadata: {str(e)}\")\n",
    "\n",
    "print(f\"\\nAll results exported to: {results_dir}\")\n",
    "print(f\"All model checkpoints saved to: {checkpoints_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. FINAL SUMMARY DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use builtin sum explicitly\n",
    "total_training_time = builtins.sum(times['duration_seconds'] for times in training_times.values())\n",
    "print(f\"Total Training Time: {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\")\n",
    "print(f\"Results saved to: {results_dir}\")\n",
    "print(f\"Model checkpoints: {checkpoints_dir}\")\n",
    "\n",
    "for task_id, task_info in tasks.items():\n",
    "    print(f\"\\n{task_info['description']}:\")\n",
    "    results = all_results[task_id]\n",
    "    \n",
    "    if task_info[\"type\"] == \"classification\":\n",
    "        if results:\n",
    "            try:\n",
    "                best_model = find_best_classification_model(results)\n",
    "                if best_model:\n",
    "                    model_name, metrics = best_model\n",
    "                    print(f\"Best: {model_name} (AUC: {metrics['AUC']:.4f})\")\n",
    "                else:\n",
    "                    print(f\"No valid models\")\n",
    "            except:\n",
    "                print(f\"Error finding best model\")\n",
    "        else:\n",
    "            print(f\"No successful models\")\n",
    "    else:\n",
    "        if results:\n",
    "            try:\n",
    "                best_model = find_best_regression_model(results)\n",
    "                if best_model:\n",
    "                    model_name, metrics = best_model\n",
    "                    print(f\"Best: {model_name} (RMSE: {metrics['RMSE']:.4f})\")\n",
    "                else:\n",
    "                    print(f\"No valid models\")\n",
    "            except:\n",
    "                print(f\"Error finding best model\")\n",
    "        else:\n",
    "            print(f\"No successful models\")\n",
    "    \n",
    "    print(f\"Training time: {training_times[task_id]['duration_minutes']:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nTraining session completed successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. HOW TO LOAD SAVED MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nTo load saved models later:\")\n",
    "print(f\"   from pyspark.ml import Pipeline\")\n",
    "print(f\"   model = Pipeline.load('path_to_model')\")\n",
    "print(f\"   # Example:\")\n",
    "for task_id in tasks.keys():\n",
    "    model_example = f\"{checkpoints_dir}/{task_id}_{timestamp}/logistic_regression\"\n",
    "    print(f\"   # model = Pipeline.load('{model_example}')\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52e391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
